{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2a6c574",
   "metadata": {},
   "source": [
    "# Treinamento de Modelo para Extração de Dados de Recibos Fiscais\n",
    "\n",
    "Este notebook demonstra o fluxo de trabalho completo para preparar dados e treinar um modelo de reconhecimento de entidades nomeadas (NER) para extrair informações de recibos fiscais, como:\n",
    "- Tipo de documento\n",
    "- CNPJ\n",
    "- Chave de acesso\n",
    "- Data de emissão\n",
    "- Valor total\n",
    "- Número do documento\n",
    "- Série\n",
    "\n",
    "O processo envolve as seguintes etapas:\n",
    "1. Instalação das dependências necessárias\n",
    "2. Extração dos textos dos recibos do arquivo dataset.dart\n",
    "3. Preparação e limpeza dos dados\n",
    "4. Criação de anotações para NER\n",
    "5. Treinamento de um modelo BERT pré-treinado para português\n",
    "6. Avaliação do modelo\n",
    "7. Uso do modelo para extrair informações de novos recibos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36693034",
   "metadata": {},
   "source": [
    "## 1. Instalação das Dependências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a5ffd3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in ./.venv/lib/python3.13/site-packages (2.2.4)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.13/site-packages (2.2.3)\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.13/site-packages (1.6.1)\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.13/site-packages (2.6.0)\n",
      "Requirement already satisfied: transformers in ./.venv/lib/python3.13/site-packages (4.51.3)\n",
      "Requirement already satisfied: datasets in ./.venv/lib/python3.13/site-packages (3.5.0)\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.13/site-packages (3.10.1)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.13/site-packages (4.67.1)\n",
      "Requirement already satisfied: seqeval in ./.venv/lib/python3.13/site-packages (1.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./.venv/lib/python3.13/site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.13/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.13/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.13/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.13/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.13/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.13/site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.13/site-packages (from torch) (78.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.13/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.13/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in ./.venv/lib/python3.13/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.13/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.13/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.13/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.13/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.13/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.13/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.venv/lib/python3.13/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.venv/lib/python3.13/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.13/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./.venv/lib/python3.13/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in ./.venv/lib/python3.13/site-packages (from datasets) (3.11.16)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.13/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.13/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.13/site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.13/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.13/site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.13/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.venv/lib/python3.13/site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.13/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.13/site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.13/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.13/site-packages (from aiohttp->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.13/site-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.13/site-packages (from aiohttp->datasets) (1.20.0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.13/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.13/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.13/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.13/site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "# Instalar as dependências necessárias\n",
    "!pip install numpy pandas scikit-learn torch transformers datasets matplotlib tqdm seqeval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8f90f0",
   "metadata": {},
   "source": [
    "## 2. Importações e Configurações Iniciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae8aaaab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matheus.oliveira/Documents/custom_model/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForTokenClassification, \n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification\n",
    ")\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from seqeval.metrics import classification_report, f1_score\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "\n",
    "# Configurações\n",
    "MODEL_NAME = \"neuralmind/bert-base-portuguese-cased\"  # Modelo pré-treinado em português\n",
    "OUTPUT_DIR = \"receipt_ner_model\"\n",
    "MAX_LENGTH = 512\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 10\n",
    "SEED = 42\n",
    "\n",
    "# Configurar as sementes para reprodutibilidade\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb2c2c3",
   "metadata": {},
   "source": [
    "## 3. Preparação do Dataset\n",
    "\n",
    "### 3.1 Definição das Entidades e Constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fcb9b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define as entidades que queremos extrair\n",
    "ENTITIES = [\n",
    "    \"TIPO_DOCUMENTO\",\n",
    "    \"CNPJ\",\n",
    "    \"CHAVE_ACESSO\",\n",
    "    \"DATA_EMISSAO\",\n",
    "    \"VALOR_TOTAL\",\n",
    "    \"NUMERO_DOCUMENTO\",\n",
    "    \"SERIE\"\n",
    "]\n",
    "\n",
    "# Defina as etiquetas para o modelo NER\n",
    "LABELS = [\n",
    "    \"O\",  # Outside (não é uma entidade)\n",
    "    \"B-TIPO_DOCUMENTO\", \"I-TIPO_DOCUMENTO\",  # Tipo de documento (NFCe, NFe, SAT, CTe...)\n",
    "    \"B-CNPJ\", \"I-CNPJ\",  # CNPJ\n",
    "    \"B-CHAVE_ACESSO\", \"I-CHAVE_ACESSO\",  # Chave de acesso\n",
    "    \"B-DATA_EMISSAO\", \"I-DATA_EMISSAO\",  # Data de emissão\n",
    "    \"B-VALOR_TOTAL\", \"I-VALOR_TOTAL\",  # Valor total\n",
    "    \"B-NUMERO_DOCUMENTO\", \"I-NUMERO_DOCUMENTO\",  # Número do documento\n",
    "    \"B-SERIE\", \"I-SERIE\"  # Série do documento\n",
    "]\n",
    "\n",
    "# Mapeamento de ID para etiqueta e vice-versa\n",
    "id2label = {i: label for i, label in enumerate(LABELS)}\n",
    "label2id = {label: i for i, label in enumerate(LABELS)}\n",
    "\n",
    "# Mapeamento dos códigos de modelo de documento para seus tipos\n",
    "DOCUMENT_TYPE_MAP = {\n",
    "    '01': 'NF',\n",
    "    '02': 'NFVC',\n",
    "    '04': 'NFP',\n",
    "    '06': 'NFCE',\n",
    "    '07': 'NFST',\n",
    "    '08': 'CTRC',\n",
    "    '09': 'CTAC',\n",
    "    '10': 'CA',\n",
    "    '11': 'CTFC',\n",
    "    '13': 'BPR',\n",
    "    '14': 'BPA',\n",
    "    '15': 'BPNB',\n",
    "    '16': 'BPF',\n",
    "    '17': 'DT',\n",
    "    '18': 'RMD',\n",
    "    '20': 'OCC',\n",
    "    '21': 'NFSC',\n",
    "    '22': 'NFST',\n",
    "    '23': 'GNRE',\n",
    "    '24': 'AC',\n",
    "    '25': 'MC',\n",
    "    '26': 'CTMC',\n",
    "    '27': 'NFTFC',\n",
    "    '28': 'NFCG',\n",
    "    '29': 'NFCA',\n",
    "    '30': 'BRP',\n",
    "    '2D': 'CFECF',\n",
    "    '2E': 'BPEC',\n",
    "    '55': 'NFE',\n",
    "    '57': 'CTE',\n",
    "    '59': 'CF',\n",
    "    '60': 'CFEECF',\n",
    "    '65': 'NFCE',\n",
    "    '67': 'CTE',\n",
    "    '8B': 'CTCA',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7501e4",
   "metadata": {},
   "source": [
    "### 3.2 Extração dos Textos do Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5d7a216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando dados já preparados...\n",
      "Carregados 1 textos do arquivo dataset_prepared/raw_texts.txt.\n",
      "\n",
      "Exemplo de texto carregado:\n",
      "--------------------------------------------------------------------------------\n",
      "=== TEXTO 1 ===\n",
      "COOP COOPERATIVA DE CUNSURY AGOSTO, 3045 • INHN - Tatuo - . CNPJ57.508.426/0007-63 IE687.134.677.115 IM 25.457 208748 CUPON FISCAL ELETRONICO - SAT #| COD | DESC | QTD IUNIVL UN RSI (UL TR R) * IUL ITEM R 001 13624 002 12083 0. 352% fa 003 7891999144983 KE CREN TRAD 1cda 11390 3.08, Desconto sobre iten 004 12347 PAO FRC 8550KG 005 7834900283° 8 48 45 8.41) Total bruto de itens OTRI is descontos sobre iten PT' 6.68 4.05 9, 49 • 30 .99 10.84 10.49 46:38 Ponte: e 0 o 8. ore 0, 09 VO...\n",
      "--------------------------------------------------------------------------------\n",
      "Carregadas 27 anotações do arquivo dataset_prepared/annotations.json.\n",
      "Carregados 27 registros de dados extraídos do arquivo dataset_prepared/extracted_data.json.\n"
     ]
    }
   ],
   "source": [
    "# As the data has already been extracted and is available in dataset_prepared/,\n",
    "# we'll skip the extraction from dataset.dart and load the prepared data directly.\n",
    "\n",
    "print(\"Carregando dados já preparados...\")\n",
    "\n",
    "# Verificar se os arquivos necessários existem\n",
    "import os\n",
    "\n",
    "raw_texts_path = \"dataset_prepared/raw_texts.txt\"\n",
    "annotations_path = \"dataset_prepared/annotations.json\"\n",
    "extracted_data_path = \"dataset_prepared/extracted_data.json\"\n",
    "\n",
    "texts = []\n",
    "annotations = []\n",
    "\n",
    "# Carregar os textos brutos\n",
    "if os.path.exists(raw_texts_path):\n",
    "    with open(raw_texts_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "        texts = content.split(\"\\n\\n---SEPARATOR---\\n\\n\")\n",
    "        texts = [text.strip() for text in texts if text.strip()]\n",
    "    print(f\"Carregados {len(texts)} textos do arquivo {raw_texts_path}.\")\n",
    "    \n",
    "    # Exibir uma amostra para verificação\n",
    "    if texts:\n",
    "        print(\"\\nExemplo de texto carregado:\")\n",
    "        print(\"-\" * 80)\n",
    "        print(texts[0][:500] + \"...\" if len(texts[0]) > 500 else texts[0])\n",
    "        print(\"-\" * 80)\n",
    "else:\n",
    "    print(f\"Arquivo {raw_texts_path} não encontrado.\")\n",
    "\n",
    "# Carregar as anotações\n",
    "if os.path.exists(annotations_path):\n",
    "    with open(annotations_path, 'r', encoding='utf-8') as f:\n",
    "        annotations = json.load(f)\n",
    "    print(f\"Carregadas {len(annotations)} anotações do arquivo {annotations_path}.\")\n",
    "else:\n",
    "    print(f\"Arquivo {annotations_path} não encontrado.\")\n",
    "\n",
    "# Carregar os dados extraídos\n",
    "if os.path.exists(extracted_data_path):\n",
    "    with open(extracted_data_path, 'r', encoding='utf-8') as f:\n",
    "        extracted_data = json.load(f)\n",
    "    print(f\"Carregados {len(extracted_data)} registros de dados extraídos do arquivo {extracted_data_path}.\")\n",
    "else:\n",
    "    print(f\"Arquivo {extracted_data_path} não encontrado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08917716",
   "metadata": {},
   "source": [
    "### 3.3 Funções Auxiliares para Extração de Informações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2fd5960",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_access_key(key: str) -> bool:\n",
    "    \"\"\"\n",
    "    Verifica se a chave de acesso é válida com base nos prefixos conhecidos.\n",
    "    \"\"\"\n",
    "    # Lista de prefixos válidos para chaves de acesso\n",
    "    valid_prefixes = [\n",
    "        \"11\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\", \"21\", \"22\", \"23\", \"24\", \"25\", \n",
    "        \"26\", \"27\", \"28\", \"29\", \"31\", \"32\", \"33\", \"35\", \"41\", \"42\", \"43\", \"50\", \n",
    "        \"51\", \"52\", \"53\"\n",
    "    ]\n",
    "    \n",
    "    # Verifica se a chave tem 44 dígitos e começa com um prefixo válido\n",
    "    if len(key) == 44 and key[:2] in valid_prefixes:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def extract_date_info_from_key(key: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Extrai o ano e mês da chave de acesso.\n",
    "    \n",
    "    Args:\n",
    "        key: Chave de acesso de 44 dígitos\n",
    "        \n",
    "    Returns:\n",
    "        Tupla contendo (ano, mês) da chave\n",
    "    \"\"\"\n",
    "    if len(key) != 44:\n",
    "        return None, None\n",
    "    \n",
    "    # Ano está nas posições 2-4 (2 dígitos)\n",
    "    year = key[2:4]\n",
    "    # Mês está nas posições 4-6 (2 dígitos)\n",
    "    month = key[4:6]\n",
    "    \n",
    "    # Converte para números, adiciona 2000 ao ano para ter formato de 4 dígitos\n",
    "    try:\n",
    "        year = int(year)\n",
    "        month = int(month)\n",
    "        if 0 < month <= 12:  # Valida o mês\n",
    "            # Formata para 4 dígitos (assumindo anos 2000)\n",
    "            full_year = 2000 + year\n",
    "            return str(full_year), f\"{month:02d}\"\n",
    "    except ValueError:\n",
    "        pass\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "def extract_document_type_from_key(key: str) -> str:\n",
    "    \"\"\"\n",
    "    Extrai o tipo de documento a partir da chave de acesso.\n",
    "    \n",
    "    Args:\n",
    "        key: Chave de acesso de 44 dígitos\n",
    "        \n",
    "    Returns:\n",
    "        String com o tipo de documento ou string vazia se não for reconhecido\n",
    "    \"\"\"\n",
    "    if len(key) != 44:\n",
    "        return \"\"\n",
    "    \n",
    "    # O tipo de documento está nas posições 20-22\n",
    "    model_code = key[20:22]\n",
    "    \n",
    "    # Retorna o tipo de documento correspondente ou string vazia se não encontrado\n",
    "    return DOCUMENT_TYPE_MAP.get(model_code, \"\")\n",
    "\n",
    "def clean_currency_value(value: str) -> str:\n",
    "    \"\"\"\n",
    "    Limpa e padroniza um valor monetário.\n",
    "    \n",
    "    Args:\n",
    "        value: Valor monetário como string (ex: \"114,54\", \"R$ 50.00\", etc.)\n",
    "        \n",
    "    Returns:\n",
    "        Valor padronizado como string decimal com ponto (ex: \"114.54\")\n",
    "    \"\"\"\n",
    "    if not value:\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove qualquer caractere que não seja dígito, vírgula ou ponto\n",
    "    value = re.sub(r'[^\\d,.]', '', value)\n",
    "    \n",
    "    # Substitui vírgula por ponto para padronização decimal\n",
    "    value = value.replace(',', '.')\n",
    "    \n",
    "    # Se houver mais de um ponto, mantém apenas o último (caso de milhares)\n",
    "    if value.count('.') > 1:\n",
    "        parts = value.split('.')\n",
    "        last_part = parts[-1]\n",
    "        rest = ''.join(parts[:-1]).replace('.', '')\n",
    "        value = f\"{rest}.{last_part}\"\n",
    "    \n",
    "    return value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d179c76",
   "metadata": {},
   "source": [
    "### 3.4 Extração de Anotações dos Textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20a719a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando anotações previamente criadas...\n",
      "Temos 27 anotações disponíveis para uso.\n",
      "\n",
      "Exemplo de anotação:\n",
      "--------------------------------------------------------------------------------\n",
      "ID: N/A\n",
      "Texto (primeiros 100 caracteres): COOP COOPERATIVA DE CUNSURY AGOSTO, 3045 • INHN - Tatuo - . CNPJ57.508.426/0007-63 IE687.134.677.115...\n",
      "Anotações:\n",
      "  - tipo: CF\n",
      "  - cnpj: 57508426000763\n",
      "  - chave_acesso: 35250157508426000763590004379042087486138245\n",
      "  - data_emissao: 28/01/2025\n",
      "  - valor_total_pago: 41.05\n",
      "  - numero_documento: 208748\n",
      "  - serie: \n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Como as anotações já foram criadas anteriormente e estão disponíveis em dataset_prepared/annotations.json,\n",
    "# vamos pular a etapa de criação de anotações.\n",
    "\n",
    "print(\"Usando anotações previamente criadas...\")\n",
    "\n",
    "# Verificar se as anotações foram carregadas corretamente\n",
    "if 'annotations' in locals() and annotations:\n",
    "    print(f\"Temos {len(annotations)} anotações disponíveis para uso.\")\n",
    "    \n",
    "    # Exibir um exemplo para verificação\n",
    "    if annotations:\n",
    "        example = annotations[0]\n",
    "        print(\"\\nExemplo de anotação:\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"ID: {example.get('id', 'N/A')}\")\n",
    "        print(f\"Texto (primeiros 100 caracteres): {example['text'][:100]}...\")\n",
    "        print(\"Anotações:\")\n",
    "        for key, value in example['data'].items():\n",
    "            print(f\"  - {key}: {value}\")\n",
    "        print(\"-\" * 80)\n",
    "else:\n",
    "    print(\"Nenhuma anotação disponível. Verifique se o arquivo dataset_prepared/annotations.json existe e está correto.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cef03ae",
   "metadata": {},
   "source": [
    "## 4. Preparação dos Dados para Treinamento\n",
    "\n",
    "Vamos preparar o conjunto de dados para treinar o modelo de reconhecimento de entidades (NER)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdaba68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Já existem 27 anotações carregadas. Pulando criação de novas anotações.\n"
     ]
    }
   ],
   "source": [
    "def create_annotations_for_ner(texts, extracted_data):\n",
    "    \"\"\"\n",
    "    Cria anotações para treinamento de NER a partir dos textos e dados extraídos.\n",
    "    \n",
    "    Args:\n",
    "        texts: Lista de textos de recibos\n",
    "        extracted_data: Lista de dicionários com dados extraídos\n",
    "        \n",
    "    Returns:\n",
    "        Lista de dicionários com anotações no formato adequado para NER\n",
    "    \"\"\"\n",
    "    ner_annotations = []\n",
    "    \n",
    "    for i, (text, data) in enumerate(zip(texts, extracted_data)):\n",
    "        # Criar um identificador único para o exemplo\n",
    "        example_id = f\"receipt_{i:04d}\"\n",
    "        \n",
    "        # Anotar as entidades no texto\n",
    "        annotations = {}\n",
    "        for entity_type, entity_value in data.items():\n",
    "            if entity_value:  # Se o valor não estiver vazio\n",
    "                annotations[entity_type] = entity_value\n",
    "        \n",
    "        # Adicionar ao conjunto de anotações\n",
    "        ner_annotations.append({\n",
    "            \"id\": example_id,\n",
    "            \"text\": text,\n",
    "            \"data\": annotations\n",
    "        })\n",
    "    \n",
    "    print(f\"Criadas {len(ner_annotations)} anotações para NER.\")\n",
    "    return ner_annotations\n",
    "\n",
    "# Verificar se já temos anotações carregadas\n",
    "if 'annotations' not in locals() or not annotations:\n",
    "    print(\"Criando novas anotações para NER...\")\n",
    "    \n",
    "    # Verificar se temos textos e dados extraídos disponíveis\n",
    "    if 'texts' in locals() and texts and 'extracted_data' in locals() and extracted_data:\n",
    "        if len(texts) == len(extracted_data):\n",
    "            annotations = create_annotations_for_ner(texts, extracted_data)\n",
    "            \n",
    "            # Salvar as anotações para uso futuro\n",
    "            os.makedirs(\"dataset_prepared\", exist_ok=True)\n",
    "            with open(\"dataset_prepared/annotations.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(annotations, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"Anotações salvas em dataset_prepared/annotations.json\")\n",
    "        else:\n",
    "            print(f\"Erro: número de textos ({len(texts)}) não corresponde ao número de dados extraídos ({len(extracted_data)}).\")\n",
    "    else:\n",
    "        print(\"Dados de textos ou extraídos não disponíveis. Verifique as células anteriores.\")\n",
    "else:\n",
    "    print(f\"Já existem {len(annotations)} anotações carregadas. Pulando criação de novas anotações.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df56bcd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bce19470",
   "metadata": {},
   "source": [
    "## 5. Preparação do Dataset para Treinamento com Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7f24f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nenhum dado NER disponível para conversão.\n"
     ]
    }
   ],
   "source": [
    "def convert_to_datasets_format(ner_data):\n",
    "    \"\"\"\n",
    "    Converte os dados NER para o formato esperado pela biblioteca Datasets.\n",
    "    \"\"\"\n",
    "    # Dividir em conjuntos de treino, validação e teste\n",
    "    train_data, test_data = train_test_split(ner_data, test_size=0.2, random_state=SEED)\n",
    "    train_data, val_data = train_test_split(train_data, test_size=0.25, random_state=SEED)  # 0.25 * 0.8 = 0.2\n",
    "    \n",
    "    # Converter para o formato do Datasets\n",
    "    def convert_format(data):\n",
    "        return {\n",
    "            \"tokens\": [example[\"tokens\"] for example in data],\n",
    "            \"ner_tags\": [[label2id[tag] for tag in example[\"ner_tags\"]] for example in data],\n",
    "            \"id\": [example[\"id\"] for example in data]\n",
    "        }\n",
    "    \n",
    "    train_dataset = Dataset.from_dict(convert_format(train_data))\n",
    "    val_dataset = Dataset.from_dict(convert_format(val_data))\n",
    "    test_dataset = Dataset.from_dict(convert_format(test_data))\n",
    "    \n",
    "    # Criar um DatasetDict\n",
    "    datasets = DatasetDict({\n",
    "        \"train\": train_dataset,\n",
    "        \"validation\": val_dataset,\n",
    "        \"test\": test_dataset\n",
    "    })\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "# Converter para formato de dataset\n",
    "if 'ner_data' in locals() and ner_data:\n",
    "    print(\"Convertendo para formato de dataset...\")\n",
    "    datasets = convert_to_datasets_format(ner_data)\n",
    "    \n",
    "    print(f\"Criados datasets com:\")\n",
    "    print(f\"  - {len(datasets['train'])} exemplos de treino\")\n",
    "    print(f\"  - {len(datasets['validation'])} exemplos de validação\")\n",
    "    print(f\"  - {len(datasets['test'])} exemplos de teste\")\n",
    "else:\n",
    "    print(\"Nenhum dado NER disponível para conversão.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50515be",
   "metadata": {},
   "source": [
    "## 6. Tokenização para o Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a644f42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando tokenizador para o modelo neuralmind/bert-base-portuguese-cased...\n",
      "Nenhum dataset disponível para tokenização.\n"
     ]
    }
   ],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    \"\"\"\n",
    "    Tokeniza os textos e alinha as etiquetas NER aos tokens produzidos pelo tokenizador.\n",
    "    \"\"\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    \n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        \n",
    "        for word_idx in word_ids:\n",
    "            # Tokens especiais ([CLS], [SEP], [PAD]) recebem -100 (ignorados na perda)\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # Para o primeiro token de uma palavra, use a etiqueta correspondente\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # Para subpalavras subsequentes, use -100 (ignoradas na perda)\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "            \n",
    "        labels.append(label_ids)\n",
    "    \n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Carregar o tokenizador\n",
    "print(f\"Carregando tokenizador para o modelo {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Tokenizar e alinhar as etiquetas\n",
    "if 'datasets' in locals():\n",
    "    print(\"Tokenizando datasets...\")\n",
    "    tokenized_datasets = datasets.map(\n",
    "        tokenize_and_align_labels,\n",
    "        batched=True,\n",
    "        remove_columns=datasets[\"train\"].column_names\n",
    "    )\n",
    "    print(\"Datasets tokenizados e prontos para treinamento.\")\n",
    "else:\n",
    "    print(\"Nenhum dataset disponível para tokenização.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f5aa7e",
   "metadata": {},
   "source": [
    "## 7. Definição do Modelo e Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddfc890f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cpu\n",
      "Nenhum dataset tokenizado disponível para treinamento.\n"
     ]
    }
   ],
   "source": [
    "# Verificar disponibilidade de GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "# Configuração do modelo\n",
    "if 'tokenized_datasets' in locals():\n",
    "    print(f\"Carregando modelo base {MODEL_NAME}...\")\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=len(LABELS),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id\n",
    "    )\n",
    "    model.to(device)\n",
    "    \n",
    "    # Configuração de treinamento\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        weight_decay=0.01,\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        report_to=\"none\",  # Desabilitar relatório para services externos\n",
    "    )\n",
    "    \n",
    "    # Função de cálculo de métricas\n",
    "    def compute_metrics(p):\n",
    "        predictions, labels = p\n",
    "        predictions = np.argmax(predictions, axis=2)\n",
    "        \n",
    "        # Remover tokens ignorados (-100)\n",
    "        true_predictions = [\n",
    "            [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(predictions, labels)\n",
    "        ]\n",
    "        true_labels = [\n",
    "            [id2label[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(predictions, labels)\n",
    "        ]\n",
    "        \n",
    "        # Calcular F1-score\n",
    "        f1 = f1_score(true_labels, true_predictions)\n",
    "        return {\"f1\": f1}\n",
    "    \n",
    "    # Colator de dados\n",
    "    data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "    \n",
    "    # Criar o treinador\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"validation\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    \n",
    "    print(\"Modelo configurado e pronto para treinamento.\")\n",
    "    print(\"Você pode iniciar o treinamento executando a célula abaixo.\")\n",
    "else:\n",
    "    print(\"Nenhum dataset tokenizado disponível para treinamento.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0628bd21",
   "metadata": {},
   "source": [
    "## 8. Treinar o Modelo\n",
    "\n",
    "Execute esta célula para iniciar o treinamento do modelo. Isso pode levar algum tempo, especialmente se você não tiver uma GPU disponível."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73d280ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinador não está disponível. Por favor, execute as células anteriores primeiro.\n"
     ]
    }
   ],
   "source": [
    "# Treinar o modelo\n",
    "if 'trainer' in locals():\n",
    "    print(\"Iniciando treinamento...\")\n",
    "    trainer.train()\n",
    "    print(\"Treinamento concluído!\")\n",
    "    \n",
    "    # Salvar o modelo treinado\n",
    "    trainer.save_model(OUTPUT_DIR)\n",
    "    tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "    print(f\"Modelo salvo em {OUTPUT_DIR}\")\n",
    "else:\n",
    "    print(\"Treinador não está disponível. Por favor, execute as células anteriores primeiro.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c482d3",
   "metadata": {},
   "source": [
    "## 9. Avaliar o Modelo no Conjunto de Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0862a5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinador ou datasets não disponíveis. Por favor, execute as células anteriores primeiro.\n"
     ]
    }
   ],
   "source": [
    "# Avaliar o modelo no conjunto de teste\n",
    "if 'trainer' in locals() and 'tokenized_datasets' in locals():\n",
    "    print(\"Avaliando o modelo no conjunto de teste...\")\n",
    "    test_results = trainer.evaluate(tokenized_datasets[\"test\"])\n",
    "    print(f\"Resultados da avaliação: {test_results}\")\n",
    "    \n",
    "    # Obter previsões detalhadas\n",
    "    predictions, labels, _ = trainer.predict(tokenized_datasets[\"test\"])\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    \n",
    "    # Remover tokens ignorados (-100)\n",
    "    true_predictions = [\n",
    "        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [id2label[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    # Imprimir relatório de classificação\n",
    "    print(\"Relatório de classificação:\")\n",
    "    print(classification_report(true_labels, true_predictions))\n",
    "else:\n",
    "    print(\"Treinador ou datasets não disponíveis. Por favor, execute as células anteriores primeiro.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8148c2",
   "metadata": {},
   "source": [
    "## 10. Exemplo de Uso do Modelo Treinado para Extrair Informações de Novos Recibos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df7c471c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diretório do modelo receipt_ner_model não encontrado. Execute o treinamento primeiro.\n"
     ]
    }
   ],
   "source": [
    "def extract_info_from_receipt(text, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Extrai informações de um recibo fiscal usando o modelo treinado.\n",
    "    \n",
    "    Args:\n",
    "        text: Texto do recibo\n",
    "        model: Modelo NER treinado\n",
    "        tokenizer: Tokenizador associado ao modelo\n",
    "        \n",
    "    Returns:\n",
    "        Dicionário com as informações extraídas\n",
    "    \"\"\"\n",
    "    # Tokenizar o texto\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Tokenizar para o modelo\n",
    "    inputs = tokenizer(\n",
    "        tokens,\n",
    "        is_split_into_words=True,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH\n",
    "    )\n",
    "    \n",
    "    # Mover para GPU se disponível\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "        model = model.to(\"cuda\")\n",
    "    \n",
    "    # Obter previsões\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Converter para labels\n",
    "    predictions = outputs.logits.argmax(dim=2)\n",
    "    word_ids = inputs.word_ids(batch_index=0)\n",
    "    \n",
    "    # Extrair entidades reconhecidas\n",
    "    entities = {entity: \"\" for entity in ENTITIES}\n",
    "    current_entity = None\n",
    "    current_tokens = []\n",
    "    \n",
    "    for idx, (word_idx, pred) in enumerate(zip(word_ids, predictions[0])):\n",
    "        if word_idx is None:\n",
    "            continue\n",
    "            \n",
    "        label = id2label[pred.item()]\n",
    "        \n",
    "        if label.startswith(\"B-\"):\n",
    "            # Se estava coletando tokens para outra entidade, salvá-los\n",
    "            if current_entity and current_tokens:\n",
    "                entities[current_entity] = \" \".join(current_tokens)\n",
    "                \n",
    "            # Iniciar nova entidade\n",
    "            current_entity = label[2:]  # Remover o \"B-\"\n",
    "            current_tokens = [tokens[word_idx]]\n",
    "            \n",
    "        elif label.startswith(\"I-\") and current_entity == label[2:]:\n",
    "            # Continuar adicionando tokens à entidade atual\n",
    "            current_tokens.append(tokens[word_idx])\n",
    "            \n",
    "        elif label == \"O\" and current_entity:\n",
    "            # Finalizar entidade atual\n",
    "            entities[current_entity] = \" \".join(current_tokens)\n",
    "            current_entity = None\n",
    "            current_tokens = []\n",
    "    \n",
    "    # Lidar com qualquer entidade restante no final\n",
    "    if current_entity and current_tokens:\n",
    "        entities[current_entity] = \" \".join(current_tokens)\n",
    "    \n",
    "    # Retornar as entidades encontradas\n",
    "    return entities\n",
    "\n",
    "# Carregar o modelo salvo (se disponível)\n",
    "try:\n",
    "    if os.path.exists(OUTPUT_DIR):\n",
    "        print(f\"Carregando modelo salvo de {OUTPUT_DIR}...\")\n",
    "        model = AutoModelForTokenClassification.from_pretrained(OUTPUT_DIR)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(OUTPUT_DIR)\n",
    "        print(\"Modelo carregado com sucesso!\")\n",
    "        \n",
    "        # Exemplo de uso com um texto de teste\n",
    "        if 'texts' in locals() and texts:\n",
    "            # Usar um texto do conjunto de dados que não foi usado no treinamento\n",
    "            test_text = texts[-1]  # Usar o último texto como exemplo\n",
    "            \n",
    "            print(\"\\nExtraindo informações do recibo de exemplo...\")\n",
    "            extracted_info = extract_info_from_receipt(test_text, model, tokenizer)\n",
    "            \n",
    "            print(\"\\nInformações extraídas:\")\n",
    "            for entity, value in extracted_info.items():\n",
    "                print(f\"{entity}: {value}\")\n",
    "    else:\n",
    "        print(f\"Diretório do modelo {OUTPUT_DIR} não encontrado. Execute o treinamento primeiro.\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao carregar o modelo: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c29f8c",
   "metadata": {},
   "source": [
    "## 11. Conclusão\n",
    "\n",
    "Este notebook demonstrou todo o processo de preparação de dados e treinamento de um modelo de reconhecimento de entidades nomeadas (NER) para extração de informações de recibos fiscais. O fluxo de trabalho incluiu:\n",
    "\n",
    "1. Extração dos textos do arquivo de dados\n",
    "2. Preparação e limpeza dos dados\n",
    "3. Criação de anotações para treinamento do modelo NER\n",
    "4. Preparação dos datasets para treinamento\n",
    "5. Treinamento do modelo utilizando um BERT pré-treinado para português\n",
    "6. Avaliação do modelo no conjunto de teste\n",
    "7. Uso do modelo para extrair informações de novos recibos\n",
    "\n",
    "O modelo resultante pode ser usado para automatizar a extração de informações importantes de recibos fiscais, facilitando processos de gestão financeira, contabilidade e conformidade fiscal."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
