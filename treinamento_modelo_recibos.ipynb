{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2a6c574",
   "metadata": {},
   "source": [
    "# Treinamento de Modelo para Extração de Dados de Recibos Fiscais\n",
    "\n",
    "Este notebook demonstra o fluxo de trabalho completo para preparar dados e treinar um modelo de reconhecimento de entidades nomeadas (NER) para extrair informações de recibos fiscais, como:\n",
    "- Tipo de documento\n",
    "- CNPJ\n",
    "- Chave de acesso\n",
    "- Data de emissão\n",
    "- Valor total\n",
    "- Número do documento\n",
    "- Série\n",
    "\n",
    "O processo envolve as seguintes etapas:\n",
    "1. Instalação das dependências necessárias\n",
    "2. Extração dos textos dos recibos do arquivo dataset.dart\n",
    "3. Preparação e limpeza dos dados\n",
    "4. Criação de anotações para NER\n",
    "5. Treinamento de um modelo BERT pré-treinado para português\n",
    "6. Avaliação do modelo\n",
    "7. Uso do modelo para extrair informações de novos recibos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36693034",
   "metadata": {},
   "source": [
    "## 1. Instalação das Dependências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5ffd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar as dependências necessárias\n",
    "!pip install numpy pandas scikit-learn torch transformers datasets matplotlib tqdm seqeval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8f90f0",
   "metadata": {},
   "source": [
    "## 2. Importações e Configurações Iniciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8aaaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForTokenClassification, \n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification\n",
    ")\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from seqeval.metrics import classification_report, f1_score\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "# Configurações\n",
    "MODEL_NAME = \"neuralmind/bert-base-portuguese-cased\"  # Modelo pré-treinado em português\n",
    "OUTPUT_DIR = \"receipt_ner_model\"\n",
    "MAX_LENGTH = 512\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 10\n",
    "SEED = 42\n",
    "\n",
    "# Configurar as sementes para reprodutibilidade\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb2c2c3",
   "metadata": {},
   "source": [
    "## 3. Preparação do Dataset\n",
    "\n",
    "### 3.1 Definição das Entidades e Constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcb9b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define as entidades que queremos extrair\n",
    "ENTITIES = [\n",
    "    \"TIPO_DOCUMENTO\",\n",
    "    \"CNPJ\",\n",
    "    \"CHAVE_ACESSO\",\n",
    "    \"DATA_EMISSAO\",\n",
    "    \"VALOR_TOTAL\",\n",
    "    \"NUMERO_DOCUMENTO\",\n",
    "    \"SERIE\"\n",
    "]\n",
    "\n",
    "# Defina as etiquetas para o modelo NER\n",
    "LABELS = [\n",
    "    \"O\",  # Outside (não é uma entidade)\n",
    "    \"B-TIPO_DOCUMENTO\", \"I-TIPO_DOCUMENTO\",  # Tipo de documento (NFCe, NFe, SAT, CTe...)\n",
    "    \"B-CNPJ\", \"I-CNPJ\",  # CNPJ\n",
    "    \"B-CHAVE_ACESSO\", \"I-CHAVE_ACESSO\",  # Chave de acesso\n",
    "    \"B-DATA_EMISSAO\", \"I-DATA_EMISSAO\",  # Data de emissão\n",
    "    \"B-VALOR_TOTAL\", \"I-VALOR_TOTAL\",  # Valor total\n",
    "    \"B-NUMERO_DOCUMENTO\", \"I-NUMERO_DOCUMENTO\",  # Número do documento\n",
    "    \"B-SERIE\", \"I-SERIE\"  # Série do documento\n",
    "]\n",
    "\n",
    "# Mapeamento de ID para etiqueta e vice-versa\n",
    "id2label = {i: label for i, label in enumerate(LABELS)}\n",
    "label2id = {label: i for i, label in enumerate(LABELS)}\n",
    "\n",
    "# Mapeamento dos códigos de modelo de documento para seus tipos\n",
    "DOCUMENT_TYPE_MAP = {\n",
    "    '01': 'NF',\n",
    "    '02': 'NFVC',\n",
    "    '04': 'NFP',\n",
    "    '06': 'NFCE',\n",
    "    '07': 'NFST',\n",
    "    '08': 'CTRC',\n",
    "    '09': 'CTAC',\n",
    "    '10': 'CA',\n",
    "    '11': 'CTFC',\n",
    "    '13': 'BPR',\n",
    "    '14': 'BPA',\n",
    "    '15': 'BPNB',\n",
    "    '16': 'BPF',\n",
    "    '17': 'DT',\n",
    "    '18': 'RMD',\n",
    "    '20': 'OCC',\n",
    "    '21': 'NFSC',\n",
    "    '22': 'NFST',\n",
    "    '23': 'GNRE',\n",
    "    '24': 'AC',\n",
    "    '25': 'MC',\n",
    "    '26': 'CTMC',\n",
    "    '27': 'NFTFC',\n",
    "    '28': 'NFCG',\n",
    "    '29': 'NFCA',\n",
    "    '30': 'BRP',\n",
    "    '2D': 'CFECF',\n",
    "    '2E': 'BPEC',\n",
    "    '55': 'NFE',\n",
    "    '57': 'CTE',\n",
    "    '59': 'CF',\n",
    "    '60': 'CFEECF',\n",
    "    '65': 'NFCE',\n",
    "    '67': 'CTE',\n",
    "    '8B': 'CTCA',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7501e4",
   "metadata": {},
   "source": [
    "### 3.2 Extração dos Textos do Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d7a216",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_texts_from_dart(dart_file_path: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extrai os textos do arquivo Dart.\n",
    "    \"\"\"\n",
    "    with open(dart_file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Encontra todas as strings multi-linhas no arquivo Dart\n",
    "    pattern = r\"static const \\w+ = r?'''(.*?)''';\"\n",
    "    matches = re.findall(pattern, content, re.DOTALL)\n",
    "    \n",
    "    texts = []\n",
    "    for match in matches:\n",
    "        # Normaliza o texto, removendo espaços extras e caracteres especiais\n",
    "        text = match.strip()\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        texts.append(text)\n",
    "    \n",
    "    return texts\n",
    "\n",
    "# Extrair os textos do arquivo Dart\n",
    "dart_file_path = \"dataset.dart\"  # Altere para o caminho correto se necessário\n",
    "\n",
    "try:\n",
    "    texts = extract_texts_from_dart(dart_file_path)\n",
    "    print(f\"Extraídos {len(texts)} textos do arquivo Dart.\")\n",
    "    \n",
    "    # Exibir uma amostra para verificação\n",
    "    if texts:\n",
    "        print(\"\\nExemplo de texto extraído:\")\n",
    "        print(\"-\" * 80)\n",
    "        print(texts[0][:500] + \"...\")\n",
    "        print(\"-\" * 80)\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao extrair textos: {e}\")\n",
    "    \n",
    "    # Criação de diretório para salvar os textos extraídos\n",
    "    os.makedirs(\"dataset_prepared\", exist_ok=True)\n",
    "    \n",
    "    # Salvar os textos brutos para uso posterior\n",
    "    if texts:\n",
    "        with open(\"dataset_prepared/raw_texts.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            for text in texts:\n",
    "                f.write(text + \"\\n\\n---SEPARATOR---\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08917716",
   "metadata": {},
   "source": [
    "### 3.3 Funções Auxiliares para Extração de Informações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fd5960",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_access_key(key: str) -> bool:\n",
    "    \"\"\"\n",
    "    Verifica se a chave de acesso é válida com base nos prefixos conhecidos.\n",
    "    \"\"\"\n",
    "    # Lista de prefixos válidos para chaves de acesso\n",
    "    valid_prefixes = [\n",
    "        \"11\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\", \"21\", \"22\", \"23\", \"24\", \"25\", \n",
    "        \"26\", \"27\", \"28\", \"29\", \"31\", \"32\", \"33\", \"35\", \"41\", \"42\", \"43\", \"50\", \n",
    "        \"51\", \"52\", \"53\"\n",
    "    ]\n",
    "    \n",
    "    # Verifica se a chave tem 44 dígitos e começa com um prefixo válido\n",
    "    if len(key) == 44 and key[:2] in valid_prefixes:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def extract_date_info_from_key(key: str) -> tuple:\n",
    "    \"\"\"\n",
    "    Extrai o ano e mês da chave de acesso.\n",
    "    \n",
    "    Args:\n",
    "        key: Chave de acesso de 44 dígitos\n",
    "        \n",
    "    Returns:\n",
    "        Tupla contendo (ano, mês) da chave\n",
    "    \"\"\"\n",
    "    if len(key) != 44:\n",
    "        return None, None\n",
    "    \n",
    "    # Ano está nas posições 2-4 (2 dígitos)\n",
    "    year = key[2:4]\n",
    "    # Mês está nas posições 4-6 (2 dígitos)\n",
    "    month = key[4:6]\n",
    "    \n",
    "    # Converte para números, adiciona 2000 ao ano para ter formato de 4 dígitos\n",
    "    try:\n",
    "        year = int(year)\n",
    "        month = int(month)\n",
    "        if 0 < month <= 12:  # Valida o mês\n",
    "            # Formata para 4 dígitos (assumindo anos 2000)\n",
    "            full_year = 2000 + year\n",
    "            return str(full_year), f\"{month:02d}\"\n",
    "    except ValueError:\n",
    "        pass\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "def extract_document_type_from_key(key: str) -> str:\n",
    "    \"\"\"\n",
    "    Extrai o tipo de documento a partir da chave de acesso.\n",
    "    \n",
    "    Args:\n",
    "        key: Chave de acesso de 44 dígitos\n",
    "        \n",
    "    Returns:\n",
    "        String com o tipo de documento ou string vazia se não for reconhecido\n",
    "    \"\"\"\n",
    "    if len(key) != 44:\n",
    "        return \"\"\n",
    "    \n",
    "    # O tipo de documento está nas posições 20-22\n",
    "    model_code = key[20:22]\n",
    "    \n",
    "    # Retorna o tipo de documento correspondente ou string vazia se não encontrado\n",
    "    return DOCUMENT_TYPE_MAP.get(model_code, \"\")\n",
    "\n",
    "def clean_currency_value(value: str) -> str:\n",
    "    \"\"\"\n",
    "    Limpa e padroniza um valor monetário.\n",
    "    \n",
    "    Args:\n",
    "        value: Valor monetário como string (ex: \"114,54\", \"R$ 50.00\", etc.)\n",
    "        \n",
    "    Returns:\n",
    "        Valor padronizado como string decimal com ponto (ex: \"114.54\")\n",
    "    \"\"\"\n",
    "    if not value:\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove qualquer caractere que não seja dígito, vírgula ou ponto\n",
    "    value = re.sub(r'[^\\d,.]', '', value)\n",
    "    \n",
    "    # Substitui vírgula por ponto para padronização decimal\n",
    "    value = value.replace(',', '.')\n",
    "    \n",
    "    # Se houver mais de um ponto, mantém apenas o último (caso de milhares)\n",
    "    if value.count('.') > 1:\n",
    "        parts = value.split('.')\n",
    "        last_part = parts[-1]\n",
    "        rest = ''.join(parts[:-1]).replace('.', '')\n",
    "        value = f\"{rest}.{last_part}\"\n",
    "    \n",
    "    return value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d179c76",
   "metadata": {},
   "source": [
    "### 3.4 Extração de Anotações dos Textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a719a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_annotations(texts: List[str]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Extrai as informações solicitadas dos textos e retorna um dataset estruturado\n",
    "    no formato JSON.\n",
    "    \"\"\"\n",
    "    annotations = []\n",
    "    \n",
    "    for i, text in enumerate(texts):\n",
    "        # Inicializa o dicionário de dados com valores padrão\n",
    "        data = {\n",
    "            \"tipo\": \"\",\n",
    "            \"cnpj\": \"\",\n",
    "            \"chave_acesso\": \"\",\n",
    "            \"data_emissao\": \"\",\n",
    "            \"valor_total_pago\": \"\",\n",
    "            \"numero_documento\": \"\",\n",
    "            \"serie\": \"\"\n",
    "        }\n",
    "        \n",
    "        # Extrai chave de acesso (44 dígitos) - PRIORIDADE ALTA\n",
    "        chave_patterns = [\n",
    "            r'(\\d{4}\\s\\d{4}\\s\\d{4}\\s\\d{4}\\s\\d{4}\\s\\d{4}\\s\\d{4}\\s\\d{4}\\s\\d{4}\\s\\d{4}\\s\\d{4})',  # Com espaços\n",
    "            r'(\\d{44})'  # Sem espaços\n",
    "        ]\n",
    "        \n",
    "        # Procura pela chave de acesso primeiro\n",
    "        chave_acesso = \"\"\n",
    "        for pattern in chave_patterns:\n",
    "            chave_matches = re.findall(pattern, text)\n",
    "            if chave_matches:\n",
    "                for match in chave_matches:\n",
    "                    # Remove todos os caracteres não numéricos\n",
    "                    potential_key = re.sub(r'[^0-9]', '', match)\n",
    "                    if len(potential_key) == 44 and is_valid_access_key(potential_key):\n",
    "                        chave_acesso = potential_key\n",
    "                        data[\"chave_acesso\"] = chave_acesso\n",
    "                        break\n",
    "                if chave_acesso:\n",
    "                    break\n",
    "        \n",
    "        # Variáveis para armazenar informações extraídas da chave\n",
    "        key_year, key_month = None, None\n",
    "        document_type_from_key = \"\"\n",
    "        \n",
    "        # Se encontrou a chave de acesso, extrai informações dela\n",
    "        if chave_acesso:\n",
    "            # Extrai CNPJ da chave de acesso (posições 6-20)\n",
    "            data[\"cnpj\"] = chave_acesso[6:20]\n",
    "            \n",
    "            # Extrai número do documento da chave de acesso (posições 31-37)\n",
    "            data[\"numero_documento\"] = str(int(chave_acesso[31:37]))  # Remove zeros à esquerda\n",
    "            \n",
    "            # Extrai ano e mês da chave de acesso\n",
    "            key_year, key_month = extract_date_info_from_key(chave_acesso)\n",
    "            \n",
    "            # Extrai o tipo de documento da chave de acesso\n",
    "            document_type_from_key = extract_document_type_from_key(chave_acesso)\n",
    "            if document_type_from_key:\n",
    "                data[\"tipo\"] = document_type_from_key\n",
    "        \n",
    "        # Se não encontrou o tipo de documento na chave ou não encontrou a chave,\n",
    "        # tenta identificar pelo texto (método original)\n",
    "        if not document_type_from_key:\n",
    "            tipo_documento = \"\"\n",
    "            if \"NFC-e\" in text or \"NOTA FISCAL DE CONSUMIDOR ELETRONICA\" in text or \"NFCe\" in text:\n",
    "                tipo_documento = \"NFCE\"\n",
    "            elif \"NF-e\" in text or \"NOTA FISCAL ELETRONICA\" in text or \"NFe\" in text:\n",
    "                tipo_documento = \"NFE\"\n",
    "            elif \"SAT\" in text or \"CUPOM FISCAL ELETRÔNICO - SAT\" in text:\n",
    "                tipo_documento = \"SAT\"\n",
    "            elif \"CT-e\" in text or \"CTE\" in text or \"CONHECIMENTO DE TRANSPORTE\" in text:\n",
    "                tipo_documento = \"CTE\"\n",
    "            elif \"CUPOM FISCAL\" in text:\n",
    "                tipo_documento = \"CF\"\n",
    "            elif \"DANFE\" in text:\n",
    "                tipo_documento = \"NFE\"\n",
    "            data[\"tipo\"] = tipo_documento\n",
    "        \n",
    "        # Se não encontrou a chave ou outras informações, continua com os métodos alternativos\n",
    "        if not chave_acesso:\n",
    "            # Tenta extrair CNPJ pelo método alternativo\n",
    "            cnpj_matches = re.findall(r'CNPJ[:\\s]*(\\d{2}[\\.]?\\d{3}[\\.]?\\d{3}[/\\.]?\\d{4}[-\\.]?\\d{2}|\\d{14})', text, re.IGNORECASE)\n",
    "            if cnpj_matches:\n",
    "                cnpj = cnpj_matches[0]\n",
    "                # Remove todos os caracteres não numéricos\n",
    "                cnpj = re.sub(r'[^0-9]', '', cnpj)\n",
    "                data[\"cnpj\"] = cnpj\n",
    "            \n",
    "            # Tenta extrair número do documento pelo método alternativo apenas se não foi extraído da chave\n",
    "            if not data[\"numero_documento\"]:\n",
    "                numero_patterns = [\n",
    "                    r'N[°º\\.]?[:\\s]*(?:0*)(\\d+)',\n",
    "                    r'Nº[:\\s]*(?:0*)(\\d+)',\n",
    "                    r'N[°\\.]?[:\\s]*(?:0*)(\\d+)',\n",
    "                    r'n°[:\\s]*(?:0*)(\\d+)',\n",
    "                    r'(?:NF(?:C|E)?-e|SAT)[:\\s]*(?:n[°\\.]?)?[:\\s]*(?:0*)(\\d+)',\n",
    "                    r'Extrato\\s+(?:N[°º\\.]?)?[:\\s]*(?:0*)(\\d+)'\n",
    "                ]\n",
    "                \n",
    "                for pattern in numero_patterns:\n",
    "                    numero_matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "                    if numero_matches:\n",
    "                        data[\"numero_documento\"] = numero_matches[0]\n",
    "                        break\n",
    "        \n",
    "        # Extrai data de emissão\n",
    "        data_patterns = [\n",
    "            r'Data\\s+de\\s+[Ee]miss[aã]o[:\\s]*(\\d{2}/\\d{2}/\\d{4})',\n",
    "            r'[Ee]miss[ãa]o[:\\s]*(\\d{2}/\\d{2}/\\d{4})',\n",
    "            r'DATA\\s+DE\\s+EMISSÃO[:\\s]*(\\d{2}/\\d{2}/\\d{4})',\n",
    "            r'(\\d{2}/\\d{2}/\\d{4})\\s*-\\s*\\d{2}:\\d{2}',  # Padrão data - hora\n",
    "            r'(\\d{2}/\\d{2}/\\d{4})'  # Qualquer data no formato DD/MM/AAAA\n",
    "        ]\n",
    "        \n",
    "        # Armazena todas as datas encontradas para posterior validação\n",
    "        all_dates = []\n",
    "        for pattern in data_patterns:\n",
    "            data_matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "            all_dates.extend(data_matches)\n",
    "        \n",
    "        # Se temos informações de data da chave de acesso, vamos usá-las para validar\n",
    "        if key_year and key_month and all_dates:\n",
    "            best_date = None\n",
    "            for date_str in all_dates:\n",
    "                # Extrair componentes da data encontrada (formato DD/MM/AAAA)\n",
    "                day, month, year = date_str.split('/')\n",
    "                \n",
    "                # Verifica se o mês e o ano correspondem aos da chave de acesso\n",
    "                if month == key_month and year == key_year:\n",
    "                    best_date = date_str\n",
    "                    break\n",
    "            \n",
    "            # Se encontrou uma data que corresponde à chave, usa essa\n",
    "            if best_date:\n",
    "                data[\"data_emissao\"] = best_date\n",
    "            elif all_dates:  # Caso contrário, usa a primeira data encontrada\n",
    "                data[\"data_emissao\"] = all_dates[0]\n",
    "        elif all_dates:  # Se não há dados da chave, usa a primeira data encontrada\n",
    "            data[\"data_emissao\"] = all_dates[0]\n",
    "        \n",
    "        # Extrai valor total\n",
    "        valor_patterns = [\n",
    "            r'VALOR\\s+TOTAL\\s+(?:R\\$)?\\s*([\\d\\.]+,[\\d]+)',\n",
    "            r'VALOR\\s+A\\s+PAGAR\\s+(?:R\\$)?\\s*([\\d\\.]+,[\\d]+)',\n",
    "            r'Total\\s+(?:R\\$)?\\s*([\\d\\.]+,[\\d]+)',\n",
    "            r'TOTAL\\s+(?:R\\$)?\\s*([\\d\\.]+,[\\d]+)',\n",
    "            r'Valor\\s+Total:\\s*(?:R\\$)?\\s*([\\d\\.]+,[\\d]+)',\n",
    "            r'R\\$\\s*([\\d\\.]+,[\\d]+)',  # Padrão genérico de valores em reais\n",
    "            r'([\\d\\.]+,[\\d]+)'  # Qualquer valor numérico com decimal separado por vírgula\n",
    "        ]\n",
    "        \n",
    "        for pattern in valor_patterns:\n",
    "            valor_matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "            if valor_matches:\n",
    "                # Limpa e padroniza o valor\n",
    "                valor = clean_currency_value(valor_matches[0])\n",
    "                data[\"valor_total_pago\"] = valor\n",
    "                break\n",
    "        \n",
    "        # Extrai série\n",
    "        serie_patterns = [\n",
    "            r'S[eé]rie[:\\s]*([0-9]+)',\n",
    "            r'SERIE[:\\s]*([0-9]+)',\n",
    "            r'SÉRIE[:\\s]*([0-9]+)',\n",
    "            r'Serie[:\\s]*([0-9]+)'\n",
    "        ]\n",
    "        \n",
    "        for pattern in serie_patterns:\n",
    "            serie_matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "            if serie_matches:\n",
    "                data[\"serie\"] = serie_matches[0]\n",
    "                break\n",
    "        \n",
    "        # Adiciona os dados extraídos à lista de anotações\n",
    "        annotations.append({\"id\": i, \"text\": text, \"annotations\": data})\n",
    "    \n",
    "    return annotations\n",
    "\n",
    "# Criar as anotações a partir dos textos extraídos\n",
    "if 'texts' in locals() and texts:\n",
    "    print(\"Gerando anotações a partir dos textos...\")\n",
    "    annotations = create_annotations(texts)\n",
    "    print(f\"Criadas {len(annotations)} anotações.\")\n",
    "    \n",
    "    # Salvar anotações\n",
    "    with open(\"dataset_prepared/annotations.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(annotations, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # Extrair os dados para um arquivo JSON separado\n",
    "    extracted_data = []\n",
    "    for ann in annotations:\n",
    "        extracted_data.append({\n",
    "            \"id\": ann[\"id\"],\n",
    "            **ann[\"annotations\"]\n",
    "        })\n",
    "    \n",
    "    with open(\"dataset_prepared/extracted_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(extracted_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(\"Dados extraídos e salvos em dataset_prepared/\")\n",
    "else:\n",
    "    print(\"Nenhum texto disponível para criar anotações.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cef03ae",
   "metadata": {},
   "source": [
    "## 4. Preparação dos Dados para Treinamento\n",
    "\n",
    "Vamos preparar o conjunto de dados para treinar o modelo de reconhecimento de entidades (NER)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df56bcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_annotations_to_ner_format(annotations: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Converte as anotações no formato JSON para o formato NER adequado para treinamento.\n",
    "    Cada token receberá uma tag usando o esquema BIO (Begin, Inside, Outside).\n",
    "    \n",
    "    Args:\n",
    "        annotations: Lista de anotações obtidas do método create_annotations\n",
    "        \n",
    "    Returns:\n",
    "        Lista de dicionários no formato {\"tokens\": [...], \"ner_tags\": [...], ...}\n",
    "    \"\"\"\n",
    "    ner_data = []\n",
    "    \n",
    "    for ann in tqdm(annotations, desc=\"Convertendo para formato NER\"):\n",
    "        text = ann[\"text\"]\n",
    "        ann_data = ann[\"annotations\"]\n",
    "        \n",
    "        # Tokenizar o texto (aqui, por simplicidade, dividimos por espaço)\n",
    "        tokens = text.split()\n",
    "        ner_tags = [\"O\"] * len(tokens)  # Inicialmente, todos os tokens são \"Outside\"\n",
    "        \n",
    "        # Tentar etiquetar cada entidade no texto\n",
    "        for entity_type, entity_value in ann_data.items():\n",
    "            if not entity_value:  # Se o valor da entidade estiver vazio, pular\n",
    "                continue\n",
    "                \n",
    "            # Mapear os nomes de entidades no JSON para os nomes de entidades no NER\n",
    "            entity_map = {\n",
    "                \"tipo\": \"TIPO_DOCUMENTO\",\n",
    "                \"cnpj\": \"CNPJ\",\n",
    "                \"chave_acesso\": \"CHAVE_ACESSO\",\n",
    "                \"data_emissao\": \"DATA_EMISSAO\",\n",
    "                \"valor_total_pago\": \"VALOR_TOTAL\",\n",
    "                \"numero_documento\": \"NUMERO_DOCUMENTO\",\n",
    "                \"serie\": \"SERIE\"\n",
    "            }\n",
    "            \n",
    "            if entity_type not in entity_map:\n",
    "                continue\n",
    "                \n",
    "            ner_entity = entity_map[entity_type]\n",
    "            \n",
    "            # Procurar o valor da entidade no texto\n",
    "            # Pode ser necessário ajustar essa lógica para entidades mais complexas\n",
    "            value_tokens = entity_value.split()\n",
    "            if not value_tokens:\n",
    "                continue\n",
    "                \n",
    "            for i in range(len(tokens) - len(value_tokens) + 1):\n",
    "                # Verificar se os tokens correspondem ao valor (ignorando case)\n",
    "                if all(tokens[i+j].lower() == value_tokens[j].lower() for j in range(len(value_tokens))):\n",
    "                    # Marcar o primeiro token como B (Begin)\n",
    "                    ner_tags[i] = f\"B-{ner_entity}\"\n",
    "                    # Marcar os tokens restantes como I (Inside)\n",
    "                    for j in range(1, len(value_tokens)):\n",
    "                        ner_tags[i+j] = f\"I-{ner_entity}\"\n",
    "        \n",
    "        # Adicionar o exemplo ao conjunto de dados NER\n",
    "        ner_data.append({\n",
    "            \"id\": ann[\"id\"],\n",
    "            \"tokens\": tokens,\n",
    "            \"ner_tags\": ner_tags\n",
    "        })\n",
    "    \n",
    "    return ner_data\n",
    "\n",
    "# Carregar as anotações se não estiverem disponíveis\n",
    "if 'annotations' not in locals() or not annotations:\n",
    "    try:\n",
    "        with open(\"dataset_prepared/annotations.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "            annotations = json.load(f)\n",
    "        print(f\"Carregadas {len(annotations)} anotações do arquivo.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao carregar anotações: {e}\")\n",
    "        annotations = []\n",
    "\n",
    "# Converter anotações para formato NER\n",
    "if annotations:\n",
    "    ner_data = convert_annotations_to_ner_format(annotations)\n",
    "    print(f\"Convertidos {len(ner_data)} exemplos para formato NER.\")\n",
    "    \n",
    "    # Exibir um exemplo para verificação\n",
    "    if ner_data:\n",
    "        example = ner_data[0]\n",
    "        print(\"\\nExemplo de dados no formato NER:\")\n",
    "        print(\"-\" * 80)\n",
    "        for token, tag in zip(example[\"tokens\"][:20], example[\"ner_tags\"][:20]):\n",
    "            print(f\"{token} -> {tag}\")\n",
    "        print(\"...\")\n",
    "        print(\"-\" * 80)\n",
    "else:\n",
    "    print(\"Nenhuma anotação disponível para conversão.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce19470",
   "metadata": {},
   "source": [
    "## 5. Preparação do Dataset para Treinamento com Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f24f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_datasets_format(ner_data):\n",
    "    \"\"\"\n",
    "    Converte os dados NER para o formato esperado pela biblioteca Datasets.\n",
    "    \"\"\"\n",
    "    # Dividir em conjuntos de treino, validação e teste\n",
    "    train_data, test_data = train_test_split(ner_data, test_size=0.2, random_state=SEED)\n",
    "    train_data, val_data = train_test_split(train_data, test_size=0.25, random_state=SEED)  # 0.25 * 0.8 = 0.2\n",
    "    \n",
    "    # Converter para o formato do Datasets\n",
    "    def convert_format(data):\n",
    "        return {\n",
    "            \"tokens\": [example[\"tokens\"] for example in data],\n",
    "            \"ner_tags\": [[label2id[tag] for tag in example[\"ner_tags\"]] for example in data],\n",
    "            \"id\": [example[\"id\"] for example in data]\n",
    "        }\n",
    "    \n",
    "    train_dataset = Dataset.from_dict(convert_format(train_data))\n",
    "    val_dataset = Dataset.from_dict(convert_format(val_data))\n",
    "    test_dataset = Dataset.from_dict(convert_format(test_data))\n",
    "    \n",
    "    # Criar um DatasetDict\n",
    "    datasets = DatasetDict({\n",
    "        \"train\": train_dataset,\n",
    "        \"validation\": val_dataset,\n",
    "        \"test\": test_dataset\n",
    "    })\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "# Converter para formato de dataset\n",
    "if 'ner_data' in locals() and ner_data:\n",
    "    print(\"Convertendo para formato de dataset...\")\n",
    "    datasets = convert_to_datasets_format(ner_data)\n",
    "    \n",
    "    print(f\"Criados datasets com:\")\n",
    "    print(f\"  - {len(datasets['train'])} exemplos de treino\")\n",
    "    print(f\"  - {len(datasets['validation'])} exemplos de validação\")\n",
    "    print(f\"  - {len(datasets['test'])} exemplos de teste\")\n",
    "else:\n",
    "    print(\"Nenhum dado NER disponível para conversão.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50515be",
   "metadata": {},
   "source": [
    "## 6. Tokenização para o Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a644f42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    \"\"\"\n",
    "    Tokeniza os textos e alinha as etiquetas NER aos tokens produzidos pelo tokenizador.\n",
    "    \"\"\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    \n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        \n",
    "        for word_idx in word_ids:\n",
    "            # Tokens especiais ([CLS], [SEP], [PAD]) recebem -100 (ignorados na perda)\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # Para o primeiro token de uma palavra, use a etiqueta correspondente\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # Para subpalavras subsequentes, use -100 (ignoradas na perda)\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "            \n",
    "        labels.append(label_ids)\n",
    "    \n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Carregar o tokenizador\n",
    "print(f\"Carregando tokenizador para o modelo {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Tokenizar e alinhar as etiquetas\n",
    "if 'datasets' in locals():\n",
    "    print(\"Tokenizando datasets...\")\n",
    "    tokenized_datasets = datasets.map(\n",
    "        tokenize_and_align_labels,\n",
    "        batched=True,\n",
    "        remove_columns=datasets[\"train\"].column_names\n",
    "    )\n",
    "    print(\"Datasets tokenizados e prontos para treinamento.\")\n",
    "else:\n",
    "    print(\"Nenhum dataset disponível para tokenização.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f5aa7e",
   "metadata": {},
   "source": [
    "## 7. Definição do Modelo e Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfc890f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar disponibilidade de GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "# Configuração do modelo\n",
    "if 'tokenized_datasets' in locals():\n",
    "    print(f\"Carregando modelo base {MODEL_NAME}...\")\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=len(LABELS),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id\n",
    "    )\n",
    "    model.to(device)\n",
    "    \n",
    "    # Configuração de treinamento\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        weight_decay=0.01,\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        report_to=\"none\",  # Desabilitar relatório para services externos\n",
    "    )\n",
    "    \n",
    "    # Função de cálculo de métricas\n",
    "    def compute_metrics(p):\n",
    "        predictions, labels = p\n",
    "        predictions = np.argmax(predictions, axis=2)\n",
    "        \n",
    "        # Remover tokens ignorados (-100)\n",
    "        true_predictions = [\n",
    "            [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(predictions, labels)\n",
    "        ]\n",
    "        true_labels = [\n",
    "            [id2label[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(predictions, labels)\n",
    "        ]\n",
    "        \n",
    "        # Calcular F1-score\n",
    "        f1 = f1_score(true_labels, true_predictions)\n",
    "        return {\"f1\": f1}\n",
    "    \n",
    "    # Colator de dados\n",
    "    data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "    \n",
    "    # Criar o treinador\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"validation\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    \n",
    "    print(\"Modelo configurado e pronto para treinamento.\")\n",
    "    print(\"Você pode iniciar o treinamento executando a célula abaixo.\")\n",
    "else:\n",
    "    print(\"Nenhum dataset tokenizado disponível para treinamento.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0628bd21",
   "metadata": {},
   "source": [
    "## 8. Treinar o Modelo\n",
    "\n",
    "Execute esta célula para iniciar o treinamento do modelo. Isso pode levar algum tempo, especialmente se você não tiver uma GPU disponível."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d280ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar o modelo\n",
    "if 'trainer' in locals():\n",
    "    print(\"Iniciando treinamento...\")\n",
    "    trainer.train()\n",
    "    print(\"Treinamento concluído!\")\n",
    "    \n",
    "    # Salvar o modelo treinado\n",
    "    trainer.save_model(OUTPUT_DIR)\n",
    "    tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "    print(f\"Modelo salvo em {OUTPUT_DIR}\")\n",
    "else:\n",
    "    print(\"Treinador não está disponível. Por favor, execute as células anteriores primeiro.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c482d3",
   "metadata": {},
   "source": [
    "## 9. Avaliar o Modelo no Conjunto de Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0862a5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliar o modelo no conjunto de teste\n",
    "if 'trainer' in locals() and 'tokenized_datasets' in locals():\n",
    "    print(\"Avaliando o modelo no conjunto de teste...\")\n",
    "    test_results = trainer.evaluate(tokenized_datasets[\"test\"])\n",
    "    print(f\"Resultados da avaliação: {test_results}\")\n",
    "    \n",
    "    # Obter previsões detalhadas\n",
    "    predictions, labels, _ = trainer.predict(tokenized_datasets[\"test\"])\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    \n",
    "    # Remover tokens ignorados (-100)\n",
    "    true_predictions = [\n",
    "        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [id2label[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    # Imprimir relatório de classificação\n",
    "    print(\"Relatório de classificação:\")\n",
    "    print(classification_report(true_labels, true_predictions))\n",
    "else:\n",
    "    print(\"Treinador ou datasets não disponíveis. Por favor, execute as células anteriores primeiro.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8148c2",
   "metadata": {},
   "source": [
    "## 10. Exemplo de Uso do Modelo Treinado para Extrair Informações de Novos Recibos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7c471c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info_from_receipt(text, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Extrai informações de um recibo fiscal usando o modelo treinado.\n",
    "    \n",
    "    Args:\n",
    "        text: Texto do recibo\n",
    "        model: Modelo NER treinado\n",
    "        tokenizer: Tokenizador associado ao modelo\n",
    "        \n",
    "    Returns:\n",
    "        Dicionário com as informações extraídas\n",
    "    \"\"\"\n",
    "    # Tokenizar o texto\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Tokenizar para o modelo\n",
    "    inputs = tokenizer(\n",
    "        tokens,\n",
    "        is_split_into_words=True,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH\n",
    "    )\n",
    "    \n",
    "    # Mover para GPU se disponível\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
    "        model = model.to(\"cuda\")\n",
    "    \n",
    "    # Obter previsões\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Converter para labels\n",
    "    predictions = outputs.logits.argmax(dim=2)\n",
    "    word_ids = inputs.word_ids(batch_index=0)\n",
    "    \n",
    "    # Extrair entidades reconhecidas\n",
    "    entities = {entity: \"\" for entity in ENTITIES}\n",
    "    current_entity = None\n",
    "    current_tokens = []\n",
    "    \n",
    "    for idx, (word_idx, pred) in enumerate(zip(word_ids, predictions[0])):\n",
    "        if word_idx is None:\n",
    "            continue\n",
    "            \n",
    "        label = id2label[pred.item()]\n",
    "        \n",
    "        if label.startswith(\"B-\"):\n",
    "            # Se estava coletando tokens para outra entidade, salvá-los\n",
    "            if current_entity and current_tokens:\n",
    "                entities[current_entity] = \" \".join(current_tokens)\n",
    "                \n",
    "            # Iniciar nova entidade\n",
    "            current_entity = label[2:]  # Remover o \"B-\"\n",
    "            current_tokens = [tokens[word_idx]]\n",
    "            \n",
    "        elif label.startswith(\"I-\") and current_entity == label[2:]:\n",
    "            # Continuar adicionando tokens à entidade atual\n",
    "            current_tokens.append(tokens[word_idx])\n",
    "            \n",
    "        elif label == \"O\" and current_entity:\n",
    "            # Finalizar entidade atual\n",
    "            entities[current_entity] = \" \".join(current_tokens)\n",
    "            current_entity = None\n",
    "            current_tokens = []\n",
    "    \n",
    "    # Lidar com qualquer entidade restante no final\n",
    "    if current_entity and current_tokens:\n",
    "        entities[current_entity] = \" \".join(current_tokens)\n",
    "    \n",
    "    # Retornar as entidades encontradas\n",
    "    return entities\n",
    "\n",
    "# Carregar o modelo salvo (se disponível)\n",
    "try:\n",
    "    if os.path.exists(OUTPUT_DIR):\n",
    "        print(f\"Carregando modelo salvo de {OUTPUT_DIR}...\")\n",
    "        model = AutoModelForTokenClassification.from_pretrained(OUTPUT_DIR)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(OUTPUT_DIR)\n",
    "        print(\"Modelo carregado com sucesso!\")\n",
    "        \n",
    "        # Exemplo de uso com um texto de teste\n",
    "        if 'texts' in locals() and texts:\n",
    "            # Usar um texto do conjunto de dados que não foi usado no treinamento\n",
    "            test_text = texts[-1]  # Usar o último texto como exemplo\n",
    "            \n",
    "            print(\"\\nExtraindo informações do recibo de exemplo...\")\n",
    "            extracted_info = extract_info_from_receipt(test_text, model, tokenizer)\n",
    "            \n",
    "            print(\"\\nInformações extraídas:\")\n",
    "            for entity, value in extracted_info.items():\n",
    "                print(f\"{entity}: {value}\")\n",
    "    else:\n",
    "        print(f\"Diretório do modelo {OUTPUT_DIR} não encontrado. Execute o treinamento primeiro.\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao carregar o modelo: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c29f8c",
   "metadata": {},
   "source": [
    "## 11. Conclusão\n",
    "\n",
    "Este notebook demonstrou todo o processo de preparação de dados e treinamento de um modelo de reconhecimento de entidades nomeadas (NER) para extração de informações de recibos fiscais. O fluxo de trabalho incluiu:\n",
    "\n",
    "1. Extração dos textos do arquivo de dados\n",
    "2. Preparação e limpeza dos dados\n",
    "3. Criação de anotações para treinamento do modelo NER\n",
    "4. Preparação dos datasets para treinamento\n",
    "5. Treinamento do modelo utilizando um BERT pré-treinado para português\n",
    "6. Avaliação do modelo no conjunto de teste\n",
    "7. Uso do modelo para extrair informações de novos recibos\n",
    "\n",
    "O modelo resultante pode ser usado para automatizar a extração de informações importantes de recibos fiscais, facilitando processos de gestão financeira, contabilidade e conformidade fiscal."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
